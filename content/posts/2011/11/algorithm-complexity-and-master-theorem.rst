算法的复杂度与Master定理
########################
:date: 2011-11-16 14:16
:modified: 2011-11-16 14:16
:author: Calf
:category: 算法
:tags: Algorithm Complexity, Master Theorem
:keywords: Master定理, 上界, 下界, 函数渐进阶, 时间复杂度, 算法复杂度
:slug: algorithm-complexity-and-master-theorem
:featured_image: http://www.gocalf.com/blog/images/2011/11/master_theorem.png
:summary: 平时设计或者阅读一个算法的时候，必然会提到算法的复杂度（包括时间复杂度和空间复杂度）。比如我们说一个二分查找算法的平均时间复杂度为O(log n)，快速排序可能是O(n log n)。那这里的O是什么意思？这样的表达是否准确呢？今天来复习一下与算法复杂度相关的知识：函数渐进阶；记号O、Ω、θ和o；Master定理。
:depends: mathjax

平时设计或者阅读一个算法的时候，必然会提到算法的复杂度（包括时间复杂度和空间复杂度）。比如我们说一个二分查找算法的平均时间复杂度为O(log
n)，快速排序可能是O(n log
n)。那这里的O是什么意思？这样的表达是否准确呢？

今天来复习一下与算法复杂度相关的知识：函数渐进阶，记号O、Ω、θ和o；Master定理。

.. more

先插一句，在算法复杂度分析中，log通常表示以2为底的对数。

算法复杂度（算法复杂性）是用来衡量算法运行所需要的计算机资源（时间、空间）的量。通常我们利用渐进性态来描述算法的复杂度。

用n表示问题的规模，T(n)表示某个给定算法的复杂度。所谓渐进性态就是令n→∞时，T(n)中增长最快的那部分。严格的定义是：如果存在\ :math:`\widetilde{T}(n)`\ ，当n→∞时，有

.. math::

    \frac{T(n)-\widetilde{T}(n)}{T(n)} \to 0

就说\ :math:`\widetilde{T}(n)`\ 是T(n)当n→∞时的渐进性态。

比如T(n) = 2 \* n ^ 2 + n log n + 3，那么显然它的渐进性态是 2 \* n ^
2，因为当n→∞时，后两项的增长速度要慢的多，可以忽略掉。引入渐进性态是为了简化算法复杂度的表达式，只考虑其中的主要因素。当比较两个算法复杂度的时候，如果他们的渐进复杂度的阶不相同，那只需要比较彼此的阶（忽略常数系数）就可以了。

总之，分析算法复杂度的时候，并不用严格演算出一个具体的公式，而是只需要分析当问题规模充分大的时候，复杂度在渐进意义下的阶。记号O、Ω、θ和o可以帮助我们了解函数渐进阶的大小。

假设有两个函数f(n)和g(n)，都是定义在正整数集上的正函数。上述四个记号的含义分别是：

-   f(n) = O(g(n))：:math:`\exists c>0,n_0\in\mathbb{N},\forall n\geq n_0,f(n)\leq c g(n)`\ ；f的阶\ **不高于**\ g的阶。
-   f(n) = Ω(g(n))：:math:`\exists c>0,n_0\in\mathbb{N},\forall n\geq n_0,f(n)\geq c g(n)`\ ；f的阶\ **不低于**\ g的阶。
-   f(n) = θ(g(n))：:math:`\iff f(n)=O(g(n))\&\&f(n)=\Omega(g(n))`\ ；f的阶\ **等于**\ g的阶。
-   f(n) = o(g(n))：:math:`\forall\varepsilon > 0,\exists n_0\in \mathbb{N},\forall n\geq n_0,f(n)/g(n) < \varepsilon`\ ；f的阶\ **低于**\ g的阶。

可见，记号O给出了函数f(n)在渐进意义下的上界（但不一定是最小的），相反，记号Ω给出的是下界（不一定是最大的）。如果上界与下界相同，表示f(n)和g(n)在渐进意义下是同阶的（θ），亦即复杂度一样。

列举一些常见的函数之间的渐进阶的关系：

-   :math:`\log n!=\Theta(n\log n)`
-   :math:`\log n^2=\Theta(\log n)`
-   :math:`\log n^2=O(\sqrt n)`
-   :math:`n=\Omega(\log^2n)`
-   :math:`\log^2n=\Omega(\log n)`
-   :math:`2^n=\Omega(n^2)`
-   :math:`2^n=O(3^n)`
-   :math:`n!=o(n^n)`
-   :math:`2^n=o(n!)`

有些人可能会把这几个记号跟算法的最坏、最好、平均情况复杂度混淆，它们有区别，也有一定的联系。

即使问题的规模相同，随着输入数据本身属性的不同，算法的处理时间也可能会不同。于是就有了最坏情况、最好情况和平均情况下算法复杂度的区别。它们从不同的角度反映了算法的效率，各有用处，也各有局限。

有时候也可以利用最坏情况、最好情况下算法复杂度来粗略地估计算法的性能。比如某个算法在最坏情况下时间复杂度为θ(n
^ 2)，最好情况下为θ(n)，那这个算法的复杂度一定是O(n ^
2)、Ω(n)的。也就是说n ^ 2是该算法复杂度的上界，n是其下界。

接下来看看Master定理。

有些算法在处理一个较大规模的问题时，往往会把问题拆分成几个子问题，对其中的一个或多个问题递归地处理，并在分治之前或之后进行一些预处理、汇总处理。这时候我们可以得到关于这个算法复杂度的一个递推方程，求解此方程便能得到算法的复杂度。其中很常见的一种递推方程就是这样的：

设常数a >= 1，b > 1，f(n)为函数，T(n)为非负整数，T(n) = a T(n / b) +
f(n)，则有：

#. 若\ :math:`f(n)=O(n^{\log_b a-\varepsilon}),\varepsilon > 0`\ ，那么\ :math:`T(n)=\Theta(n^{\log_b a})`\ 。
#. 若\ :math:`f(n)=\Theta(n^{\log_b a})`\ ，那么\ :math:`T(n)=\Theta(n^{\log_b a}\log n)`\ 。
#. 若\ :math:`f(n)=\Omega(n^{\log_b a+\varepsilon}),\varepsilon > 0`\ ，并且对于某个常数c < 1和充分大的n有\ :math:`a f(n/b)\leq c f(n)`\ ，那么\ :math:`T(n)=\Theta(f(n))`\ 。

比如常见的二分查找算法，时间复杂度的递推方程为T(n) = T(n / 2) +
θ(1)，显然有\ :math:`n^{\log_b a}=n^0=\Theta(1)`\ ，满足Master定理第二条，可以得到其时间复杂度为T(n)
= θ(log n)。

再看一个例子，T(n) = 9 T(n / 3) + n，可知\ :math:`n^{\log_b a}=n^2`\ ，令ε取1，显然满足Master定理第一条，可以得到T(n) = θ(n ^
2)。

来一个稍微复杂一点儿例子，T(n) = 3 T(n / 4) + n log
n。:math:`n^{\log_b a}=O(n^{0.793})`\ ，取ε = 0.2，显然当c = 3 /
4时，对于充分大的n可以满足a \* f(n / b) = 3 \* (n / 4) \* log(n / 4) <=
(3 / 4) \* n \* log n = c \* f(n)，符合Master定理第三条，因此求得T(n)
= θ(n log n)。

运用Master定理的时候，有一点一定要\ **特别注意**\ ，就是第一条和第三条中的ε必须\ **大于零**\ 。如果无法找到大于零的ε，就不能使用这两条规则。

举个例子，T(n) = 2 T(n / 2) + n log n。可知\ :math:`n^{\log_b a}=n^1`\ ，而f(n) = n log
n，显然不满足Master定理第二条。但对于第一条和第三条，也无法找到大于零的ε使得\ :math:`n \log n=O(n^{1-\varepsilon})`\ 或者\ :math:`n \log n=\Omega(n^{1+\varepsilon})`\ ，因此不能用Master定理求解，只能寻求别的方式求解。比如可以利用递归树求出该算法的复杂度为\ :math:`T(n)=O(n \log^2{n})`\ 。简单的说一下计算过程：

递归树的建立过程，就像是模拟算法的递推过程。树根对应的是输入的规模为n的问题，在递归处理子问题之外，还需要n
log
n的处理时间。然后根据递推公式给根节点添加子节点，每个子节点对应一个子问题。这里需要两个子节点，每个节点处理规模为n
/ 2的问题，分别需要(n / 2) \* log(n / 2)的时间。因此在第二层一共需要n \*
(log n -
1)的时间。第三层节点就是将第二层的两个节点继续分裂开，得到四个各需要(n /
4) \* log(n / 4)时间的节点，总的时间消耗为n \* (log n -
2)。依此类推，第k（设树根为k = 0）层有2 ^ k的节点，总的时间为n \* (log n
- k)。而且可以知道，这棵树总共有log
n层（最后一层每个节点只处理规模为1的子问题，无须再分治）。最后将每一层消耗的时间累加起来，得到：

.. math::

    \sum_{k=0}^{\log n}{n(\log n-k)}=\frac{1}{2}n\log n(\log n + 1)=O(n\log^2{n})
